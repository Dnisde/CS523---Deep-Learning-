{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-423a698f8046>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image as pil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import networks\n",
    "from utils import download_model_if_doesnt_exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_model = \"mono_640x192\"\n",
    "model_path = \"./log/finetuned_mono/models/\"\n",
    "\n",
    "class model_evaluation:\n",
    "\n",
    "    # Original Monodepth2 model name: model_name = \"mono_640x192\"\n",
    "    # Your pretrained model name: self_model_name = \"weights_19\"\n",
    "    # Your test image: image_path = \"assets/test_image.jpg\"\n",
    "\n",
    "    def __init__(self, ground_truth_model_name, model_name, model_path, image_path, GT):\n",
    "        self.GT_model = ground_truth_model_name\n",
    "        self.model = model_name\n",
    "        self.path = model_path\n",
    "        self.image_path = image_path\n",
    "        if GT == 1:\n",
    "            download_model_if_doesnt_exist(ground_truth_model_name)\n",
    "            self.encoderPath = os.path.join(\"models\", ground_truth_model_name, \"encoder.pth\")\n",
    "            self.depth_decoderPath = os.path.join(\"models\", ground_truth_model_name, \"depth.pth\")\n",
    "        else:\n",
    "            self.encoderPath = os.path.join(model_path, model_name, \"encoder.pth\")\n",
    "            self.depth_decoderPath = os.path.join(model_path, model_name, \"depth.pth\")\n",
    "\n",
    "    def load_Pretrained_Model(self, model):\n",
    "        # Initialize Encoder and Decoder Structure\n",
    "        encoder = networks.ResnetEncoder(18, False)\n",
    "        depth_decoder = networks.DepthDecoder(num_ch_enc=encoder.num_ch_enc, scales=range(4))\n",
    "        # LOADING PRETRAINED MODEL\n",
    "        loaded_dict_enc = torch.load(self.encoderPath, map_location='cpu')\n",
    "        filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}\n",
    "        encoder.load_state_dict(filtered_dict_enc)\n",
    "\n",
    "        loaded_dict = torch.load(self.depth_decoderPath, map_location='cpu')\n",
    "        depth_decoder.load_state_dict(loaded_dict)\n",
    "\n",
    "        # Note!!!!!!!: Modify here: .eval() ??\n",
    "        encoder.eval()\n",
    "        depth_decoder.eval()\n",
    "\n",
    "        return encoder, depth_decoder\n",
    "\n",
    "    def load_preprocess(self):\n",
    "        # Load test image from image dataset and doing pre-processing:\n",
    "        image_path = self.image_path\n",
    "\n",
    "        input_image = pil.open(image_path).convert('RGB')\n",
    "        original_width, original_height = input_image.size\n",
    "        loaded_dict_enc = torch.load(self.encoderPath, map_location='cpu')\n",
    "        feed_height = loaded_dict_enc['height']\n",
    "        feed_width = loaded_dict_enc['width']\n",
    "        # Resize the image into certain dimension:\n",
    "        input_image_resized = input_image.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "\n",
    "        input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)\n",
    "\n",
    "        return input_image_pytorch, original_width, original_height\n",
    "\n",
    "    def prediction(self):\n",
    "        # Get Specific Trained Encoder and Decoder from the network, and visualize the performance:\n",
    "        encoder, decoder = self.load_Pretrained_Model(self.encoderPath, self.depth_decoderPath)\n",
    "        # Using the path to preprocess test image dataset:\n",
    "        input_image, input_width, input_height = self.load_preprocess(self.encoderPath, self.depth_decoderPath)\n",
    "        with torch.no_grad():\n",
    "            features = encoder(input_image)\n",
    "            outputs = decoder(features)\n",
    "        disp = outputs[(\"disp\", 0)]\n",
    "        return disp, input_image, input_width, input_height\n",
    "\n",
    "    def depth_visualize(self):\n",
    "        # Call Prediction:\n",
    "        disp, input_image, width, height = self.prediction()\n",
    "        disp_resized = torch.nn.functional.interpolate(disp,\n",
    "                                                       (height, width), mode=\"bilinear\",\n",
    "                                                       align_corners=False)\n",
    "        # Saving color-mapped depth image\n",
    "        disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    "        vmax = np.percentile(disp_resized_np, 95)\n",
    "\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.subplot(211)\n",
    "        plt.imshow(input_image)\n",
    "        plt.title(\"Input\", fontsize=22)\n",
    "        plt.axis('off');\n",
    "\n",
    "        plt.subplot(212)\n",
    "        plt.imshow(disp_resized_np, cmap='magma', vmax=vmax)\n",
    "        plt.title(\"Disparity prediction\", fontsize=22)\n",
    "        plt.axis('off');\n",
    "\n",
    "\n",
    "    def confusion_matrix(self, model_name):\n",
    "        data = {'y_Actual': [1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0],\n",
    "                'y_Predicted': [1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0]\n",
    "                }\n",
    "\n",
    "        df = pd.DataFrame(data, columns=['y_Actual', 'y_Predicted'])\n",
    "        print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    original_model = model_evaluation(ground_model, \"weights_19\", model_path, 1)\n",
    "    original_model.depth_visualize()\n",
    "    our_model =  model_evaluation(ground_model, \"weights_19\", model_path, 0)\n",
    "    our_model.depth_visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
